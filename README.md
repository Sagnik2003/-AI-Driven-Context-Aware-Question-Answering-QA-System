# Flood-Response AI Pipeline — README.md


# Flood-Response AI Pipeline

End-to-end demo pipeline that produces **safe, grounded** responses to disaster/relief style queries.  
It combines: Bayesian intent reasoning, search-based retrieval (BFS + A\*), a planner, an RL policy for decision refinement, and an LLM-based final-answer generator with safety grounding.

---

## Table of contents

- [Project overview](#project-overview)  
- [Repository layout](#repository-layout)  
- [Requirements](#requirements)  
- [Installation](#installation)  
- [Quickstart — run demo](#quickstart---run-demo)  
- [API / Programmatic usage](#api--programmatic-usage)  
- [Modules description](#modules-description)  
- [RL training & persistence](#rl-training--persistence)  
- [LLM integration & safety](#llm-integration--safety)  
- [Design notes & best practices](#design-notes--best-practices)  
- [Troubleshooting](#troubleshooting)  
- [Next steps / Ideas](#next-steps--ideas)  
- [License](#license)

---

## Project overview

This project demonstrates a pipeline to handle user queries about disasters (floods, health, compensation, pests).  
Key properties:

- **Grounded** — final answers are strictly based on official knowledge snippets from a small KB.
- **Safe** — if KB lacks necessary info, the system admits it and recommends safe actions (clarify / escalate / contact emergency services).
- **Composable** — retrieval, planning, RL, and LLM modules are separate and can be replaced independently.

---

## Repository layout

```

.
├─ pipeline.py                # (optional) all-in-one orchestrator (BN+Planner+RL integration)
├─ search_retrieval.py        # Module B: KB, BFS and A* retrieval, retrieve_with_bn()
├─ planning_module.py         # Planner implementation (GraphPlan + POP planner)
├─ rl_policy.py               # RL module: Q-learning, policy API
├─ module5_llm.py             # Final answer generator (prompt engineering + backends)
├─ AI_project(theory).pdf     # (optional) referenced KB provenance file
├─ demo_plans.json            # (generated by demo runs)
├─ README.md                  # This file

````

> **Note:** The retrieval logic (KB, TF-IDF, BFS/A*) must be in `search_retrieval.py` and **imported** by planner/RL modules to avoid duplication.

---

## Requirements

- Python 3.9+ recommended
- Core (required):
  - `scikit-learn` (TF-IDF & cosine similarity)
  - `numpy` (used by scikit-learn)
- Optional (for real LLM backends):
  - `openai` (if using OpenAI API)
  - `transformers` & `torch` (for local generation with Hugging Face models)

Install core deps:

```bash
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install --upgrade pip
pip install scikit-learn numpy
````

Optional:

```bash
pip install openai
pip install transformers torch
```

---

## Installation

1. Clone or copy the project files into a folder.
2. Create & activate a virtual env (recommended).
3. Install the dependencies above.
4. Ensure `search_retrieval.py` exists and exports:

   * `retrieve_with_bn(query)` (returns retrieval results)
   * `KB_SNIPPETS`, `KB_GRAPH_ADJ` (if you want direct access)
   * (Other helper names `VECTORIZER`, `TFIDF_MATRIX`, `IDS` are optional)

---

## Quickstart — run demo

If you kept the single `pipeline.py`:

```bash
python pipeline.py
```

If modules are split:

* Planner demo

```bash
python planning_module.py
```

* Module 5 demo (LLM prompt & mock generation)

```bash
python module5_llm.py
```

Demo outputs are written to `demo_plans.json`.

---

## API / Programmatic usage

Example usage (in your Python code):

```python
from pipeline import plan_for_query, get_default_rl_policy
from module5_llm import generate_final_answer

# 1. Plan (BN + retrieval + planning)
plan = plan_for_query("Is it safe to drink water here?", call_bn=True, call_retriever=True)

# 2. Optionally consult RL policy
rl = get_default_rl_policy()
rl_dec = rl.decide_action_for_plan_result(plan)
print("RL decision:", rl_dec)

# 3. Generate final text answer (mock LLM by default)
llm_out = generate_final_answer(plan, llm_backend='auto', print_output=False)
print(llm_out['final_text'])
```

**Single-query output control:** `plan_for_query(..., print_output=True)` prints a compact per-query summary. Modules are otherwise silent when imported.

---

## Modules description

### Module A — Bayesian Intent Reasoning

* Functions:

  * `extract_features(query)` → `{QueryType, KeywordAmbiguity, Urgency, Location}`
  * `posterior_intent(features)` → posterior probabilities over intents
* Used by Retriever & Planner for context.

### Module B — Search-based Retrieval (`search_retrieval.py`)

* KB format: `KB_SNIPPETS` (list of snippet dicts).
* Graph adjacency: `KB_GRAPH_ADJ`.
* Search methods:

  * `bfs_search(query, posterior=..., top_intents=...)`
  * `a_star_search(query, posterior=..., top_intents=...)`
  * `retrieve_with_bn(query)` — top-level: runs BN -> BFS & A* -> selects snippet.

### Module C — Planner

* GraphPlan: forward expansion over action layers.
* POPPlanner: partial-order planner to compute valid orderings.
* `plan_for_query(query, call_bn=True, call_retriever=True)` returns a structured `plan_result` with:

  * `features`, `posterior`, `final_plan` (ordered steps), `decision`, `provenance`, etc.

### Module D — RL Policy (Q-learning)

* Small discrete state-space `(intent_idx, conf_bin, location_idx, risk_bin)`.
* Actions: `DirectAnswer`, `AskClarification`, `EscalateToHuman`, `GenerateExplanation`.
* API:

  * `train_q_table(...)` → Q-table dict
  * `RLPolicy(Q_table=None).decide_action_for_plan_result(plan_result)` → RL decision object

### Module E — LLM Final Answer (`module5_llm.py`)

* Constructs a safety-first prompt using:

  * user query, BN features, retrieval snippet(s), planner decision
* Backends:

  * `openai` (if `OPENAI_API_KEY` set and package installed)
  * `transformers` (local models)
  * `mock` (deterministic template — default)
* Enforces grounding: if LLM output is not grounded, falls back to quoting snippet or admitting lack of info.

---

## RL training & persistence

Train Q-table and save:

```python
from rl_policy import train_q_table, save_q_table
Q = train_q_table(max_epochs=5000)
save_q_table(Q, "q_table.pkl")
```

Load and use:

```python
from rl_policy import load_q_table, RLPolicy
Q = load_q_table("q_table.pkl")
policy = RLPolicy(Q_table=Q)
```

`RLPolicy` will train lazily if no Q is provided.

---

## LLM integration & safety

* `module5_llm.generate_final_answer(plan_result, llm_backend='auto')` picks an available backend (OpenAI → transformers → mock).
* **Safety constraints enforced:**

  * Prompt marks official snippet(s) as the only source of truth.
  * If snippet doesn't answer the query, module returns a safe transparent response and suggests next steps (clarify / escalate / emergency contacts).
  * If LLM output is not grounded, module falls back to quoting the snippet verbatim.

**To enable OpenAI backend:**

```bash
export OPENAI_API_KEY="sk-..."           # or set in your environment
pip install openai
```

---

## Design notes & best practices

* **Single-responsibility modules** — keep retrieval, planning, RL, and LLM separate.
* **No duplication** — retrieval & KB must live in `search_retrieval.py`; other modules import from it.
* **Silent imports** — modules do not print on import; use explicit flags `print_output`/`verbose` to debug.
* **Provenance** — `plan_result` and LLM outputs include snippet IDs and source file references to enable audits.
* **Testing** — use `demo_plans.json` for integration tests, and unit tests for planner logic and RL mapping.

---

## Troubleshooting

* **`ImportError: search_retrieval.py not found`**
  Create `search_retrieval.py` with the retrieval code (KB_SNIPPETS, retrieval functions). See the retrieval section in earlier examples.

* **Missing packages (`sklearn`, `transformers`)**
  Install with `pip install scikit-learn` or `pip install transformers torch`.

* **OpenAI errors**
  Ensure `OPENAI_API_KEY` is set and valid.

* **Planner decisions not expected**
  Tweak thresholds in `build_actions_for_context()` (BN confidence thresholds) or tune RL reward function.

---

## Next steps / Ideas

* Add a REST API with FastAPI to expose `plan_for_query` and `generate_final_answer`.
* Add a small UI for operators to review/override plans and to supply human feedback for RL training.
* Move KB into a CSV/DB and implement a KB management UI.
* Replace the `mock` LLM with a small local instruction-tuned model for offline demos (requires GPU).

---

## License

This code is provided for research & educational purposes. If you plan to deploy in production, review model usage terms, safety, and legal/regulatory compliance.

---

If you want, I can:

* produce a ready-to-drop `search_retrieval.py` (exact retrieval code) so the pipeline runs immediately; or
* convert this README to a `README.md` file and include it in a zip you can download.

```
```
